{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8f168d-bbed-47b3-9a21-7f64dddf9369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 清理完成！新文件已保存为：final_dataset_clean.csv\n"
     ]
    }
   ],
   "source": [
    "#构建模型\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #我们使用TF-IDF\n",
    "\n",
    "df = pd.read_csv(\"E:/作业/杂物/web日志分析/final_dataset_400k.csv\")\n",
    "\n",
    "y = df[\"label\"]\n",
    "\n",
    "path = df[\"path\"].fillna(\"\")\n",
    "status = df[\"status\"].fillna(0)\n",
    "method = df[\"method\"].fillna(\"\")\n",
    "size = df[\"size\"].fillna(0)\n",
    "referer = df[\"referer\"].fillna(\"\")\n",
    "user_agent = df[\"user_agent\"].fillna(\"\")\n",
    "\n",
    "vec_path = TfidfVectorizer(max_features=5000)\n",
    "vec_ua = TfidfVectorizer(max_features=5000)\n",
    "vec_referer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "x_path = vec_path.fit_transform(path)\n",
    "x_ua = vec_ua.fit_transform(user_agent)\n",
    "x_referer =vec_referer.fit_transform(referer)\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')  # One-Hot 编码\n",
    "x_method = ohe.fit_transform(method.values.reshape(-1, 1))\n",
    "\n",
    "x_status= status.values.reshape(-1, 1)   # 数值特征转成二维\n",
    "x_size= size.values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "x = hstack([x_path, x_ua, x_referer, x_method, x_status, x_size])  # 合并所有特征必须使用稀疏矩阵拼接\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 模型3:XGBoost\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "params={\n",
    "    'objective':'binary:logistic', #目标函数，我们明显是二分类用 binary:logistic\n",
    "    'eval_metric':'logloss', #评估指标，这里用 logloss\n",
    "    'eta': 0.05, #学习率，值越小越稳越慢默认0.1\n",
    "    'max_depth': 8, #每棵树的深度，越深越复杂一般4-6复杂可到8\n",
    "    'subsample': 1.0, #样本采样比例，防止过拟合\n",
    "    'colsample_bytree': 0.8, #特征采样比例\n",
    "}  #不是特别理解好像就是不同的参数,感觉很难记住\n",
    "\n",
    "y_np = np.array(y_train)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_np) \n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "y_pred_prob = bst.predict(dtest)   # 输出是概率\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "print(\"XGBoost：\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad325b6c-9b74-4e49-8d17-c05d7c58606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"对未标注数据进行预测...\")  # 和前边一模一样的操作只不过没有y而已\n",
    "df = pd.read_csv(\"E:/作业/杂物/web日志分析/final_dataset_clean.csv\")\n",
    "\n",
    "path = df[\"path\"].fillna(\"\")\n",
    "status = df[\"status\"].fillna(0)\n",
    "method = df[\"method\"].fillna(\"\")\n",
    "size = df[\"size\"].fillna(0)\n",
    "referer = df[\"referer\"].fillna(\"\")\n",
    "user_agent = df[\"user_agent\"].fillna(\"\")\n",
    "\n",
    "x_path = vec_path.transform(path)\n",
    "x_ua = vec_ua.transform(user_agent)\n",
    "x_referer = vec_referer.transform(referer)\n",
    "\n",
    "x_method = ohe.transform(method.values.reshape(-1, 1))\n",
    "\n",
    "x_status = status.values.reshape(-1, 1)\n",
    "x_size = size.values.reshape(-1, 1)\n",
    "\n",
    "X_unlabeled = hstack([x_path, x_ua, x_referer, x_method, x_status, x_size])# 合并所有特征成稀疏矩阵\n",
    "dtest = xgb.DMatrix(X_unlabeled)\n",
    "probs = bst.predict(dtest) # 开始预测\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "high_confidence_idx = np.where((probs > 0.95) | (probs < 0.05))[0]  # 选出置信度高的索引\n",
    "\n",
    "pseudo_labels_high_conf = (probs[high_confidence_idx] > 0.5).astype(int)  # 置信度对应的伪标签（概率>0.5为1，否则0）\n",
    "\n",
    "X_pseudo = X_unlabeled[high_confidence_idx]  # 从无标签特征中选出高置信度样本\n",
    "\n",
    "y_pseudo = pseudo_labels_high_conf  # 把伪标签加入标签集合\n",
    "\n",
    "X_train_new = vstack([X_train, X_pseudo])\n",
    "y_train_new = np.concatenate([y_train, y_pseudo])  # 将伪标签样本加入训练集（用稀疏矩阵垂直堆叠）\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain_new = xgb.DMatrix(X_train_new, label=y_train_new)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 1.0,\n",
    "    'colsample_bytree': 0.8,\n",
    "}\n",
    "\n",
    "bst_new = xgb.train(params, dtrain_new, num_boost_round=100)\n",
    "\n",
    "max_iters = 3\n",
    "X_train_iter = X_train\n",
    "y_train_iter = y_train\n",
    "X_unlabeled_iter = X_unlabeled\n",
    "\n",
    "for i in range(max_iters):\n",
    "    print(f\"开始第{i+1}轮半监督训练\")\n",
    "    dtrain_iter = xgb.DMatrix(X_train_iter, label=y_train_iter)\n",
    "    bst_iter = xgb.train(params, dtrain_iter, num_boost_round=100)\n",
    "    \n",
    "    dtest_iter = xgb.DMatrix(X_unlabeled_iter)\n",
    "    probs_iter = bst_iter.predict(dtest_iter)\n",
    "    \n",
    "    high_conf_idx = np.where((probs_iter > 0.95) | (probs_iter < 0.05))[0]\n",
    "    if len(high_conf_idx) == 0:\n",
    "        print(\"无更多高置信度样本，停止迭代\")\n",
    "        break\n",
    "    \n",
    "    pseudo_labels_iter = (probs_iter[high_conf_idx] > 0.5).astype(int)\n",
    "    X_pseudo_iter = X_unlabeled_iter[high_conf_idx]\n",
    "    \n",
    "    # 加入训练集\n",
    "    X_train_iter = vstack([X_train_iter, X_pseudo_iter])\n",
    "    y_train_iter = np.concatenate([y_train_iter, pseudo_labels_iter])\n",
    "    \n",
    "    # 从无标签数据中剔除已用样本\n",
    "    mask = np.ones(X_unlabeled_iter.shape[0], dtype=bool)\n",
    "    mask[high_conf_idx] = False\n",
    "    X_unlabeled_iter = X_unlabeled_iter[mask]\n",
    "    \n",
    "    print(f\"第{i+1}轮加入了{len(high_conf_idx)}个伪标签样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4141e-0406-4b3a-bf99-3530da2e1407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ae415-ab7d-45d4-8af2-68e6669dce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain_new = xgb.DMatrix(X_train_new, label=y_train_new)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 1.0,\n",
    "    'colsample_bytree': 0.8,\n",
    "}\n",
    "\n",
    "bst_new = xgb.train(params, dtrain_new, num_boost_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6859e2-2bac-4d61-ad93-6ff4514c7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 3\n",
    "X_train_iter = X_train\n",
    "y_train_iter = y_train\n",
    "X_unlabeled_iter = X_unlabeled\n",
    "\n",
    "for i in range(max_iters):\n",
    "    print(f\"开始第{i+1}轮半监督训练\")\n",
    "    dtrain_iter = xgb.DMatrix(X_train_iter, label=y_train_iter)\n",
    "    bst_iter = xgb.train(params, dtrain_iter, num_boost_round=100)\n",
    "    \n",
    "    dtest_iter = xgb.DMatrix(X_unlabeled_iter)\n",
    "    probs_iter = bst_iter.predict(dtest_iter)\n",
    "    \n",
    "    high_conf_idx = np.where((probs_iter > 0.95) | (probs_iter < 0.05))[0]\n",
    "    if len(high_conf_idx) == 0:\n",
    "        print(\"无更多高置信度样本，停止迭代\")\n",
    "        break\n",
    "    \n",
    "    pseudo_labels_iter = (probs_iter[high_conf_idx] > 0.5).astype(int)\n",
    "    X_pseudo_iter = X_unlabeled_iter[high_conf_idx]\n",
    "    \n",
    "    # 加入训练集\n",
    "    X_train_iter = vstack([X_train_iter, X_pseudo_iter])\n",
    "    y_train_iter = np.concatenate([y_train_iter, pseudo_labels_iter])\n",
    "    \n",
    "    # 从无标签数据中剔除已用样本\n",
    "    mask = np.ones(X_unlabeled_iter.shape[0], dtype=bool)\n",
    "    mask[high_conf_idx] = False\n",
    "    X_unlabeled_iter = X_unlabeled_iter[mask]\n",
    "    \n",
    "    print(f\"第{i+1}轮加入了{len(high_conf_idx)}个伪标签样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c89c01-ba52-45de-a373-c4aad7b41abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ee3ce1-8b74-455b-a0f4-2075dd9a914f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练初始XGBoost模型...\n",
      "初始XGBoost模型性能：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     40134\n",
      "           1       1.00      0.99      0.99     39866\n",
      "\n",
      "    accuracy                           0.99     80000\n",
      "   macro avg       0.99      0.99      0.99     80000\n",
      "weighted avg       0.99      0.99      0.99     80000\n",
      "\n",
      "读取无标签数据...\n",
      "开始半监督学习...\n",
      "\n",
      "开始第1轮半监督训练\n",
      "当前训练集大小: 320000\n",
      "当前无标签数据大小: 400000\n",
      "第1轮加入了386833个伪标签样本\n",
      "其中正样本: 193441, 负样本: 193392\n",
      "\n",
      "开始第2轮半监督训练\n",
      "当前训练集大小: 706833\n",
      "当前无标签数据大小: 13167\n",
      "第2轮加入了4196个伪标签样本\n",
      "其中正样本: 689, 负样本: 3507\n",
      "\n",
      "开始第3轮半监督训练\n",
      "当前训练集大小: 711029\n",
      "当前无标签数据大小: 8971\n",
      "第3轮加入了699个伪标签样本\n",
      "其中正样本: 126, 负样本: 573\n",
      "\n",
      "开始第4轮半监督训练\n",
      "当前训练集大小: 711728\n",
      "当前无标签数据大小: 8272\n",
      "第4轮加入了185个伪标签样本\n",
      "其中正样本: 61, 负样本: 124\n",
      "\n",
      "开始第5轮半监督训练\n",
      "当前训练集大小: 711913\n",
      "当前无标签数据大小: 8087\n",
      "第5轮加入了146个伪标签样本\n",
      "其中正样本: 43, 负样本: 103\n",
      "\n",
      "半监督学习完成，共加入392059个伪标签样本\n",
      "\n",
      "训练最终模型...\n",
      "最终模型性能：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     40134\n",
      "           1       0.99      0.99      0.99     39866\n",
      "\n",
      "    accuracy                           0.99     80000\n",
      "   macro avg       0.99      0.99      0.99     80000\n",
      "weighted avg       0.99      0.99      0.99     80000\n",
      "\n",
      "\n",
      "性能对比:\n",
      "指标\t\t初始模型\t最终模型\n",
      "准确率\t\t0.9936\t\t0.9913\n",
      "精确率\t\t0.9959\t\t0.9949\n",
      "召回率\t\t0.9913\t\t0.9877\n",
      "F1分数\t\t0.9936\t\t0.9913\n",
      "保存模型和预处理器...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 187\u001b[0m\n\u001b[0;32m    184\u001b[0m SAVE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/作业/杂物/web日志分析/models\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# 确保保存目录存在\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(SAVE_PATH, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# 保存XGBoost模型（推荐）\u001b[39;00m\n\u001b[0;32m    190\u001b[0m model_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SAVE_PATH, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msemi_supervised_xgb_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack, vstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# 读取训练数据\n",
    "df = pd.read_csv(\"E:/作业/杂物/web日志分析/final_dataset_400k.csv\")\n",
    "\n",
    "y = df[\"label\"]\n",
    "\n",
    "# 数据预处理\n",
    "path = df[\"path\"].fillna(\"\")\n",
    "status = df[\"status\"].fillna(0)\n",
    "method = df[\"method\"].fillna(\"\")\n",
    "size = df[\"size\"].fillna(0)\n",
    "referer = df[\"referer\"].fillna(\"\")\n",
    "user_agent = df[\"user_agent\"].fillna(\"\")\n",
    "\n",
    "# 文本特征向量化\n",
    "vec_path = TfidfVectorizer(max_features=5000)\n",
    "vec_ua = TfidfVectorizer(max_features=5000)\n",
    "vec_referer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "x_path = vec_path.fit_transform(path)\n",
    "x_ua = vec_ua.fit_transform(user_agent)\n",
    "x_referer = vec_referer.fit_transform(referer)\n",
    "\n",
    "# 类别特征编码\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "x_method = ohe.fit_transform(method.values.reshape(-1, 1))\n",
    "\n",
    "# 数值特征\n",
    "x_status = status.values.reshape(-1, 1)\n",
    "x_size = size.values.reshape(-1, 1)\n",
    "\n",
    "# 合并所有特征并转换为CSR格式（支持索引操作）\n",
    "x = hstack([x_path, x_ua, x_referer, x_method, x_status, x_size]).tocsr()\n",
    "\n",
    "# 分割训练测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost参数设置\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 1.0,\n",
    "    'colsample_bytree': 0.8,\n",
    "}\n",
    "\n",
    "# 训练初始模型\n",
    "print(\"训练初始XGBoost模型...\")\n",
    "y_np = np.array(y_train)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_np)\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# 在测试集上评估初始模型\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "y_pred_prob = bst.predict(dtest)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "print(\"初始XGBoost模型性能：\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 读取无标签数据\n",
    "print(\"读取无标签数据...\")\n",
    "df_unlabeled = pd.read_csv(\"E:/作业/杂物/web日志分析/final_dataset_clean.csv\")\n",
    "\n",
    "# 对无标签数据进行相同的预处理\n",
    "path_unlabeled = df_unlabeled[\"path\"].fillna(\"\")\n",
    "status_unlabeled = df_unlabeled[\"status\"].fillna(0)\n",
    "method_unlabeled = df_unlabeled[\"method\"].fillna(\"\")\n",
    "size_unlabeled = df_unlabeled[\"size\"].fillna(0)\n",
    "referer_unlabeled = df_unlabeled[\"referer\"].fillna(\"\")\n",
    "user_agent_unlabeled = df_unlabeled[\"user_agent\"].fillna(\"\")\n",
    "\n",
    "# 使用已训练的向量化器进行转换\n",
    "x_path_unlabeled = vec_path.transform(path_unlabeled)\n",
    "x_ua_unlabeled = vec_ua.transform(user_agent_unlabeled)\n",
    "x_referer_unlabeled = vec_referer.transform(referer_unlabeled)\n",
    "\n",
    "x_method_unlabeled = ohe.transform(method_unlabeled.values.reshape(-1, 1))\n",
    "\n",
    "x_status_unlabeled = status_unlabeled.values.reshape(-1, 1)\n",
    "x_size_unlabeled = size_unlabeled.values.reshape(-1, 1)\n",
    "\n",
    "# 合并无标签数据的所有特征并转换为CSR格式\n",
    "X_unlabeled = hstack([x_path_unlabeled, x_ua_unlabeled, x_referer_unlabeled, \n",
    "                     x_method_unlabeled, x_status_unlabeled, x_size_unlabeled]).tocsr()\n",
    "\n",
    "# 半监督学习迭代过程\n",
    "print(\"开始半监督学习...\")\n",
    "max_iters = 5  # 增加迭代次数\n",
    "confidence_threshold = 0.9  # 可以调整置信度阈值\n",
    "\n",
    "X_train_iter = X_train.copy().tocsr()  # 确保是CSR格式\n",
    "y_train_iter = y_train.copy()\n",
    "X_unlabeled_iter = X_unlabeled.copy().tocsr()  # 确保是CSR格式\n",
    "\n",
    "total_pseudo_samples = 0\n",
    "\n",
    "for i in range(max_iters):\n",
    "    print(f\"\\n开始第{i+1}轮半监督训练\")\n",
    "    print(f\"当前训练集大小: {X_train_iter.shape[0]}\")\n",
    "    print(f\"当前无标签数据大小: {X_unlabeled_iter.shape[0]}\")\n",
    "    \n",
    "    # 训练当前模型\n",
    "    dtrain_iter = xgb.DMatrix(X_train_iter, label=y_train_iter)\n",
    "    bst_iter = xgb.train(params, dtrain_iter, num_boost_round=100)\n",
    "    \n",
    "    # 对无标签数据进行预测\n",
    "    dtest_iter = xgb.DMatrix(X_unlabeled_iter)\n",
    "    probs_iter = bst_iter.predict(dtest_iter)\n",
    "    \n",
    "    # 选择高置信度样本\n",
    "    high_conf_idx = np.where((probs_iter > confidence_threshold) | \n",
    "                           (probs_iter < (1 - confidence_threshold)))[0]\n",
    "    \n",
    "    if len(high_conf_idx) == 0:\n",
    "        print(\"无更多高置信度样本，停止迭代\")\n",
    "        break\n",
    "    \n",
    "    # 生成伪标签\n",
    "    pseudo_labels_iter = (probs_iter[high_conf_idx] > 0.5).astype(int)\n",
    "    X_pseudo_iter = X_unlabeled_iter[high_conf_idx]\n",
    "    \n",
    "    # 将伪标签样本加入训练集\n",
    "    X_train_iter = vstack([X_train_iter, X_pseudo_iter])\n",
    "    y_train_iter = np.concatenate([y_train_iter, pseudo_labels_iter])\n",
    "    \n",
    "    # 从无标签数据中移除已使用的样本\n",
    "    mask = np.ones(X_unlabeled_iter.shape[0], dtype=bool)\n",
    "    mask[high_conf_idx] = False\n",
    "    X_unlabeled_iter = X_unlabeled_iter[mask]\n",
    "    \n",
    "    total_pseudo_samples += len(high_conf_idx)\n",
    "    print(f\"第{i+1}轮加入了{len(high_conf_idx)}个伪标签样本\")\n",
    "    print(f\"其中正样本: {np.sum(pseudo_labels_iter)}, 负样本: {len(pseudo_labels_iter) - np.sum(pseudo_labels_iter)}\")\n",
    "\n",
    "print(f\"\\n半监督学习完成，共加入{total_pseudo_samples}个伪标签样本\")\n",
    "\n",
    "# 训练最终模型\n",
    "print(\"\\n训练最终模型...\")\n",
    "dtrain_final = xgb.DMatrix(X_train_iter, label=y_train_iter)\n",
    "bst_final = xgb.train(params, dtrain_final, num_boost_round=100)\n",
    "\n",
    "# 在测试集上评估最终模型\n",
    "dtest_final = xgb.DMatrix(X_test)\n",
    "y_pred_prob_final = bst_final.predict(dtest_final)\n",
    "y_pred_final = (y_pred_prob_final > 0.5).astype(int)\n",
    "\n",
    "print(\"最终模型性能：\")\n",
    "print(classification_report(y_test, y_pred_final))\n",
    "\n",
    "# 比较初始模型和最终模型的性能\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"\\n性能对比:\")\n",
    "print(\"指标\\t\\t初始模型\\t最终模型\")\n",
    "print(f\"准确率\\t\\t{accuracy_score(y_test, y_pred):.4f}\\t\\t{accuracy_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"精确率\\t\\t{precision_score(y_test, y_pred):.4f}\\t\\t{precision_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"召回率\\t\\t{recall_score(y_test, y_pred):.4f}\\t\\t{recall_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"F1分数\\t\\t{f1_score(y_test, y_pred):.4f}\\t\\t{f1_score(y_test, y_pred_final):.4f}\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 在训练完成后保存模型和预处理器\n",
    "print(\"保存模型和预处理器...\")\n",
    "\n",
    "# 设置你想要的保存路径\n",
    "SAVE_PATH = \"E:/作业/杂物/web日志分析/models\"\n",
    "\n",
    "# 确保保存目录存在\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# 保存XGBoost模型（推荐）\n",
    "model_filename = os.path.join(SAVE_PATH, f'semi_supervised_xgb_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json')\n",
    "bst_final.save_model(model_filename)\n",
    "print(f\"XGBoost模型已保存为: {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b424d9b-f487-48fd-8aac-30f7e7887e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存模型和预处理器...\n",
      "XGBoost模型已保存为: E:/作业/杂物/web日志分析/models\\semi_supervised_xgb_model_20250805_155259.json\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 在训练完成后保存模型和预处理器\n",
    "print(\"保存模型和预处理器...\")\n",
    "\n",
    "# 设置你想要的保存路径\n",
    "SAVE_PATH = \"E:/作业/杂物/web日志分析/models\"\n",
    "\n",
    "# 确保保存目录存在\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# 保存XGBoost模型（推荐）\n",
    "model_filename = os.path.join(SAVE_PATH, f'semi_supervised_xgb_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json')\n",
    "bst_final.save_model(model_filename)\n",
    "print(f\"XGBoost模型已保存为: {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2829469c-bc0a-473d-88cd-c9abff113525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
