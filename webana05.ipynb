{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c07223c6-68f6-4cdb-a2e5-b8d17ff19415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已从原始日志中随机抽取 400000 行，保存到 E:/作业/杂物/web日志分析/unlabeled_logs_sampled.csv\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "input_path = \"E:/作业/杂物/web日志分析/log01.csv\"\n",
    "output_path = \"E:/作业/杂物/web日志分析/unlabeled_logs_sampled.csv\"\n",
    "\n",
    "sample_size = 400000  # 你要抽的行数\n",
    "reservoir = []\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < sample_size:\n",
    "            reservoir.append(line)\n",
    "        else:\n",
    "            j = random.randint(0, i)\n",
    "            if j < sample_size:\n",
    "                reservoir[j] = line\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as out:\n",
    "    out.writelines(reservoir)\n",
    "\n",
    "print(f\"✅ 已从原始日志中随机抽取 {sample_size} 行，保存到 {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0ace965-d731-49e5-9a94-93ea902eb59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     40134\n",
      "           1       1.00      0.99      0.99     39866\n",
      "\n",
      "    accuracy                           0.99     80000\n",
      "   macro avg       0.99      0.99      0.99     80000\n",
      "weighted avg       0.99      0.99      0.99     80000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#构建模型\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #我们使用TF-IDF\n",
    "\n",
    "df = pd.read_csv(\"E:/作业/杂物/web日志分析/final_dataset_400k.csv\")\n",
    "\n",
    "y = df[\"label\"]\n",
    "\n",
    "path = df[\"path\"].fillna(\"\")\n",
    "status = df[\"status\"].fillna(0)\n",
    "method = df[\"method\"].fillna(\"\")\n",
    "size = df[\"size\"].fillna(0)\n",
    "referer = df[\"referer\"].fillna(\"\")\n",
    "user_agent = df[\"user_agent\"].fillna(\"\")\n",
    "\n",
    "vec_path = TfidfVectorizer(max_features=5000)\n",
    "vec_ua = TfidfVectorizer(max_features=5000)\n",
    "vec_referer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "x_path = vec_path.fit_transform(path)\n",
    "x_ua = vec_ua.fit_transform(user_agent)\n",
    "x_referer =vec_referer.fit_transform(referer)\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')  # One-Hot 编码\n",
    "x_method = ohe.fit_transform(method.values.reshape(-1, 1))\n",
    "\n",
    "x_status= status.values.reshape(-1, 1)   # 数值特征转成二维\n",
    "x_size= size.values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "x = hstack([x_path, x_ua, x_referer, x_method, x_status, x_size])  # 合并所有特征必须使用稀疏矩阵拼接\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 模型3:XGBoost\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "params={\n",
    "    'objective':'binary:logistic', #目标函数，我们明显是二分类用 binary:logistic\n",
    "    'eval_metric':'logloss', #评估指标，这里用 logloss\n",
    "    'eta': 0.05, #学习率，值越小越稳越慢默认0.1\n",
    "    'max_depth': 8, #每棵树的深度，越深越复杂一般4-6复杂可到8\n",
    "    'subsample': 1.0, #样本采样比例，防止过拟合\n",
    "    'colsample_bytree': 0.8, #特征采样比例\n",
    "}  #不是特别理解好像就是不同的参数,感觉很难记住\n",
    "\n",
    "y_np = np.array(y_train)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_np) \n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "y_pred_prob = bst.predict(dtest)   # 输出是概率\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "print(\"XGBoost：\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eab4778d-38f3-493f-934b-9aa932bf0c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对未标注数据进行预测...\n",
      "预测完成\n",
      "预测完成\n",
      "已保存带伪标签的新数据集！\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"对未标注数据进行预测...\")  # 和前边一模一样的操作只不过没有y而已\n",
    "df = pd.read_csv(\"E:/作业/杂物/web日志分析/unlabeled_logs_sampled_with_header.csv\")\n",
    "\n",
    "path = df[\"path\"].fillna(\"\")\n",
    "status = df[\"status\"].fillna(0)\n",
    "method = df[\"method\"].fillna(\"\")\n",
    "size = df[\"size\"].fillna(0)\n",
    "referer = df[\"referer\"].fillna(\"\")\n",
    "user_agent = df[\"user_agent\"].fillna(\"\")\n",
    "\n",
    "x_path = vec_path.transform(path)\n",
    "x_ua = vec_ua.transform(user_agent)\n",
    "x_referer = vec_referer.transform(referer)\n",
    "\n",
    "x_method = ohe.transform(method.values.reshape(-1, 1))\n",
    "\n",
    "x_status = status.values.reshape(-1, 1)\n",
    "x_size = size.values.reshape(-1, 1)\n",
    "\n",
    "X_unlabeled = hstack([x_path, x_ua, x_referer, x_method, x_status, x_size])# 合并所有特征成稀疏矩阵\n",
    "dtest = xgb.DMatrix(X_unlabeled)\n",
    "probs = bst.predict(dtest) # 开始预测\n",
    "\n",
    "pseudo_labels = (probs > 0.5).astype(int)  # 概率>0.5为1，否则0\n",
    "\n",
    "df[\"label1\"] = pseudo_labels  # 把伪标签加入原始数据集\n",
    "\n",
    "print(\"预测完成\")\n",
    "# ...你已有的代码...\n",
    "\n",
    "df[\"label1\"] = pseudo_labels  # 把伪标签加入内存中DataFrame\n",
    "print(\"预测完成\")\n",
    "\n",
    "# 保存到新文件\n",
    "df.to_csv(\"E:/作业/杂物/web日志分析/unlabeled_logs_sampled_with_label1.csv\", index=False)\n",
    "print(\"已保存带伪标签的新数据集！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb61221-9d51-4bfb-996d-be18745d3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = ['ip','time','method',\"path\", \"status\", \"size\", \"referer\", \"user_agent\"]\n",
    "\n",
    "df = pd.read_csv(\"E:/作业/杂物/web日志分析/unlabeled_logs_sampled.csv\", header=None, names=cols)\n",
    "\n",
    "# 重新写回CSV，写入列名，覆盖原文件或者写新文件\n",
    "df.to_csv(\"E:/作业/杂物/web日志分析/unlabeled_logs_sampled_with_header.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff675286-503d-46bd-a6b7-874a02e34caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打标签完成\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"E:/作业/杂物/web日志分析/unlabeled_logs_sampled_with_label1.csv\")  # 读取数据\n",
    "\n",
    "df['score'] = 0\n",
    "df['label'] = 0  # 默认都是正常的，后续高分的我们再打成 1\n",
    "\n",
    "for i, row in df.iterrows():  #和上边的评分规则一模一样\n",
    "    score = 0\n",
    "    path = str(row['path']) if pd.notna(row['path']) else \"\"\n",
    "    status = str(row['status']) if pd.notna(row['status']) else \"\"\n",
    "    user_agent = str(row['user_agent']) if pd.notna(row['user_agent']) else \"\"\n",
    "\n",
    "    if len(path) > 100:\n",
    "        score += 2\n",
    "    dangerous_words = ['script', 'union', 'select', 'drop', 'admin', '..', 'passwd']\n",
    "    for word in dangerous_words:\n",
    "        if word.lower() in path.lower():\n",
    "            score += 3\n",
    "    if status.startswith('4') or status.startswith('5'):\n",
    "        score += 1\n",
    "    if len(user_agent) < 20:\n",
    "        score += 2\n",
    "    special_chars = ['<', '>', '%', '&', ';']\n",
    "    for char in special_chars:\n",
    "        if char in path:\n",
    "            score += 1\n",
    "\n",
    "    df.at[i, 'score'] = score\n",
    "    df.at[i, 'label'] = 1 if score >= 3 else 0  # 可疑打为1，正常为0\n",
    "\n",
    "\n",
    "df.to_csv(\"E:/作业/杂物/web日志分析/unlabeled_logs_sampled_with_label1and2.csv\", index=False, encoding='utf-8-sig')  # 保存为新的带伪标签的数据集\n",
    "\n",
    "print(\"打标签完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "480d9398-2428-4406-a614-1c4ad8163a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签不一致的数量：1766\n",
      "总样本数：400000\n",
      "标签不一致的比例：0.0044（0.44%）\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"E:/作业/杂物/web日志分析/unlabeled_logs_sampled_with_label1and2.csv\")\n",
    "\n",
    "# 计算两个标签不一样的样本数\n",
    "diff_count = (df['label1'] != df['label']).sum()\n",
    "\n",
    "# 总样本数\n",
    "total = len(df)\n",
    "\n",
    "# 不一样的比例\n",
    "diff_ratio = diff_count / total\n",
    "\n",
    "print(f\"标签不一致的数量：{diff_count}\")\n",
    "print(f\"总样本数：{total}\")\n",
    "print(f\"标签不一致的比例：{diff_ratio:.4f}（{diff_ratio*100:.2f}%）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa3c57-0a66-480b-9480-2d4f9adbc3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "\n",
    "high_confidence_idx = np.where((probs > 0.95) | (probs < 0.05))[0]  # 选出置信度高的索引\n",
    "\n",
    "\n",
    "pseudo_labels_high_conf = (probs[high_confidence_idx] > 0.5).astype(int)  # 置信度对应的伪标签（概率>0.5为1，否则0）\n",
    "\n",
    "\n",
    "X_pseudo = X_unlabeled[high_confidence_idx]  # 从无标签特征中选出高置信度样本\n",
    "\n",
    "\n",
    "y_pseudo = pseudo_labels_high_conf  # 把伪标签加入标签集合\n",
    "\n",
    "\n",
    "X_train_new = vstack([X_train, X_pseudo])\n",
    "y_train_new = np.concatenate([y_train, y_pseudo])  # 将伪标签样本加入训练集（用稀疏矩阵垂直堆叠）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda62624-2732-4c35-b0a3-d2f75b5beec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain_new = xgb.DMatrix(X_train_new, label=y_train_new)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 1.0,\n",
    "    'colsample_bytree': 0.8,\n",
    "}\n",
    "\n",
    "bst_new = xgb.train(params, dtrain_new, num_boost_round=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf51413-8843-40e0-8b7a-426c2240ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 3\n",
    "X_train_iter = X_train\n",
    "y_train_iter = y_train\n",
    "X_unlabeled_iter = X_unlabeled\n",
    "\n",
    "for i in range(max_iters):\n",
    "    print(f\"开始第{i+1}轮半监督训练\")\n",
    "    dtrain_iter = xgb.DMatrix(X_train_iter, label=y_train_iter)\n",
    "    bst_iter = xgb.train(params, dtrain_iter, num_boost_round=100)\n",
    "    \n",
    "    dtest_iter = xgb.DMatrix(X_unlabeled_iter)\n",
    "    probs_iter = bst_iter.predict(dtest_iter)\n",
    "    \n",
    "    high_conf_idx = np.where((probs_iter > 0.95) | (probs_iter < 0.05))[0]\n",
    "    if len(high_conf_idx) == 0:\n",
    "        print(\"无更多高置信度样本，停止迭代\")\n",
    "        break\n",
    "    \n",
    "    pseudo_labels_iter = (probs_iter[high_conf_idx] > 0.5).astype(int)\n",
    "    X_pseudo_iter = X_unlabeled_iter[high_conf_idx]\n",
    "    \n",
    "    # 加入训练集\n",
    "    X_train_iter = vstack([X_train_iter, X_pseudo_iter])\n",
    "    y_train_iter = np.concatenate([y_train_iter, pseudo_labels_iter])\n",
    "    \n",
    "    # 从无标签数据中剔除已用样本\n",
    "    mask = np.ones(X_unlabeled_iter.shape[0], dtype=bool)\n",
    "    mask[high_conf_idx] = False\n",
    "    X_unlabeled_iter = X_unlabeled_iter[mask]\n",
    "    \n",
    "    print(f\"第{i+1}轮加入了{len(high_conf_idx)}个伪标签样本\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
